\documentclass{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{interval}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{graphicx}
%--\renewcommand{\arraystretch}{2.5}
%--\arrayrulecolor[HTML]{DB5800}
%--\usepackage{float}
%--\usepackage{longtable}
%--\usepackage{rotating}
%--\usepackage{booktabs}
%--\usepackage{caption}

\usepackage{listings}

\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\DeclareUnicodeCharacter{2212}{-}
\begin{document}


\begin{titlepage}
      \centering
      \begin{figure}
            \begin{center}
                  \includegraphics[width=0.6\textwidth]{images/logo_polimi.png}
            \end{center}
      \end{figure}
      \vfill
      {\scshape\LARGE Numerical Analysis for Machine Learning\\Academic Year 2021 - 2022 \par}
      
      
      \vfill
      \newcommand{\HRule}{\rule{\linewidth}{0.3mm}}
      \centering
      \HRule \\[0.4cm]
      \huge  movieRecommendation\\% Title of your document
      \HRule \\
      \vspace{1cm}
      {\Large Sofia \textsc{Martellozzo} \quad Matteo \textsc{Nunziante} \par}
      {\Large(10623060) \quad (10670132) \par}
      \vfill
      {\large Professor\par
          Edie \textsc{Miglio}}
\end{titlepage}


\newpage
\renewcommand\contentsname{Contents}
\tableofcontents

\newpage

%------------------------------------------%
\section{Introduction}

With the advent of the internet today, we are witnessing an enormous information overload. This exponential growth in data results in difficulty organizing and analyzing this basic information but opens up new avenues on the paths of knowledge. The question is no longer to have the information but to find the relevant information simultaneously; from there, recommendation systems were born.\\ 
\textbf{Recommender System} is a system that seeks to predict or filter preferences according to the user’s choices. Recommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. 
Recommender systems produce a list of recommendations in any of the two ways :
\begin{itemize}
      \item \textbf{Collaborative filtering}: Collaborative filtering approaches build a model from the user’s past behavior as well as similar decisions made by other users. This model is then used to predict ratings for items that users may have an interest in.\\
      \begin{figure}[h]
            \begin{center}
                  \includegraphics[width=0.5\textwidth]{images/Collaborative filtering.png}
            \end{center}
      \end{figure}\\
      The \underline{advantages} of this approach are:
      \begin{itemize}
            \item Domain knowledge not required: The system does not required a domain knowledge because is based only on item ratings.
            \item Serendipity: The model can help users discover new interests. In isolation, the ML system may not know the user is interested in a given item, but the model might still recommend it because similar users are interested in that item. 
      \end{itemize}
      The \underline{limitations} of this approach are:
      \begin{itemize}
            \item Cold start problem: The prediction of the model for a given (user, item) pair is the dot product of the corresponding embeddings. So, if an item is not seen during training, the system can't create an embedding for it and can't query the model with this item.
            \item Sparsity: The system may find problems on predicting evaluation because of a situation in which the users evaluate a little of the total number of items available in a dataset. This creates a sparse matrix with a high number of missing values.
      \end{itemize}

      \item \textbf{Content-based filtering}: Content-based filtering approaches uses a series of discrete characteristics of an item in order to recommend additional items with similar properties. Content-based filtering methods are totally based on a description of the item and a profile of the user’s preferences. It recommends items based on the user’s past preferences.\\
      \begin{figure}[ht]
            \begin{center}
                  \includegraphics[width=0.5\textwidth]{images/Content-based filtering.png}
            \end{center}
      \end{figure}\\
      The \underline{advantages} of this approach are:
      \begin{itemize}
            \item User autonomy:The model doesn't need any data about other users, since the recommendations are specific to this user. This makes it easier to scale to a large number of users.\\The model can capture the specific interests of a user, and can recommend niche items that very few other users are interested in.
            \item Immediate consideration of a new item: The model does not need the evaluation of all movie by a user, because it can be recommended without being evaluated.
      \end{itemize}
      The \underline{limitations} of this approach are:
      \begin{itemize}
            \item Content too specific: The model can only make recommendations based on existing interests of the user. In other words, the model has limited ability to expand on the users' existing interests.
            \item Big scale information: Since the feature representation of the items are hand-engineered to some extent, this technique requires a lot of domain knowledge. The items must be enouth detailed descripted and a user must evaluate several items before the system can iterpret its preferences.
      \end{itemize}
\end{itemize}
It is also possible to combine these two class of recommendation in order to overcome some limitations they faced. This type of approach is called \textbf{Hybrid Recommendation}.\\
In this project the items that has been evaluated are movies, and their scores are the rating that the users gave after they whatched them.

\newpage

%------------------------------------------%
\section{Objectives}

The purpose of this project is to develop a recommender system, which predicts the ratings of a user towards a domain-specific item. In this case, this domain-specific item is a movie, and the main focus of our recommendation system is to filter and predict only those movies that a user would prefer. The approach chosen to develop this system is a hybrid one. At first, a collaborative filtering technique has been used to make predictions on movies' ratings deducted from similarities between users. Then with the content-based filtering technique some movies' ratings are predicted, based on similarities between movies. A second approach uses the power of the machine learning developing neural networks to make predictions. The based knowledge of this system derives a data set that is described thoroughly in the next section, while in the fourth section there is a step-by-step description of the algorithm, followed by an evaluation of its performance and an example of output it generates.

%------------------------------------------%
\section{Dataset description}
The data provided are stored in two CSV files named movies.csv and ratings.csv.

\subsection{Movies}
In this dataset are stored information about the movies. The content is structured in 3 different columns : 
\begin{itemize}
      \item the \textbf{id}
      \item the \textbf{title}
      \item a list of \textbf{genres}
\end{itemize}
each row represents a movie.\\ In it are stored 10329 movies with 20 different genres:\\ Western, Documentary, Children, Crime, Film-Noir, Comedy, Adventure, Fantasy, Horror, Thriller, Mystery, Sci-Fi, Musical, Romance, Action, Animation, IMAX, Drama, War, not-defined. \\
In the following image there is a representation of it:
\begin{figure}[ht]
      \begin{center}
            \includegraphics[width=0.5\textwidth]{images/movies data.png}
      \end{center}
      \caption{movies.csv}
      \label{fig:movies.csv}
\end{figure}

\subsection{Ratings}
Here are stored all the ratings that the users gave to the movies from 0.5 to 5. The 4 columns are filled with respectively: 
\begin{itemize}
      \item the id of a \textbf{user}
      \item the id of a \textbf{movie}
      \item the value of the \textbf{rate} given by the user on the movie
      \item the \textbf{timestamp} (day and time) of the evaluation
\end{itemize}
Counting the different userId, there are 668 users that overall made 105339 ratings. \\
To follow a representation of it:
\begin{figure}[ht]
      \begin{center}
            \includegraphics[width=0.4\textwidth]{images/ratings data.png}
      \end{center}
      \caption{ratings.csv}
      \label{fig:ratings.csv}
\end{figure}\\
It's also reported the plot of the distribution of the rating data:
\begin{figure}[ht]
      \begin{center}
            \includegraphics[width=0.7\textwidth]{images/rating distributions.png}
      \end{center}
      \caption{data distribution}
      \label{fig:data distribution}
\end{figure}

\newpage

%------------------------------------------%
\section{Algorithm description}
In this section a detailed step-by-step description of the algorithm adopted is provided.\\
At first, the data has been loaded using the \textsl{pandas} library and then they have been organized in matrices.\\
Before building the matrices, the data has been split:
\begin{itemize}
      \item 80\% as training set
      \item 20\% as testing set
\end{itemize}
It is necessary to shuffle the data before, or in the test set would be only the data about lasts users; the analysis would not be correct.\\ \\
The first matrix called \textbf{ratings matrix} have all the 668 \textbf{user ids} as rows, 10329 \textbf{movie ids} as columns and as values the \textbf{ratings} that users gave to the movies (the ones missing are set to 0.0).\\
\begin{table}[ht]
      \begin{center}
            \begin{tabular}{ | c | c | c | c | c | c | c |} 
              \hline
              \rowcolor{lightgray}  & 1 & 2 & 3 & ... & 148626 & 149532 \\ 
              \hline
               \cellcolor{lightgray}1 & 0.0 & 0.0 & 0.0 &  & 0.0 & 0.0 \\ 
              \hline
              \cellcolor{lightgray}2 & 0.0 & 0.0 & 2.0 &  & 0.0 & 0.0 \\ 
              \hline
              \cellcolor{lightgray}3 & 0.0 & 0.0 & 0.0 &  & 0.0 & 0.0\\
              \hline
              \cellcolor{lightgray}4 & 0.0 & 0.0 & 0.0 &  & 0.0 & 0.0\\
              \hline
              \cellcolor{lightgray}... &  &  &  &  &  & \\ 
              \hline
              \cellcolor{lightgray}668 & 0.0 & 3.0 & 0.0 &  & 4.5 & 0.0\\
              \hline
            \end{tabular}
            \caption{Ratings matrix}
            \label{tab: Ratings matrix}
      \end{center}
\end{table}

The second matrix keeps track of all the genres associated with a movie: it has \textbf{movie ids} as rows, \textbf{genres} as columns, and as \textbf{values} "1" if the movie is associated to a genre, 0 otherwise. \\

\begin{equation}
      \label{eqn:movieMatrix}
      CorrelationMatrix_{i,j} = \left \{
            \begin{aligned}
                  &1 && \text{if movie i is of genre j}\\
                  &0 && \text{otherwise}
            \end{aligned} \right.
\end{equation} \\ 

\begin{table}[ht]
      \begin{center}
            \begin{tabular}{ | c | c | c | c | c | c | c |} 
              \hline
              \rowcolor{lightgray}  & Comedy & Drama & Horror & ... & Fantasy & Sci-Fi \\ 
              \hline
               \cellcolor{lightgray}1 & 1 & 0 & 0 &  & 1 & 0 \\ 
              \hline
              \cellcolor{lightgray}2 & 0 & 0 & 0 &  & 1 & 0 \\ 
              \hline
              \cellcolor{lightgray}3 & 1 & 0 & 0 &  & 0 & 0\\
              \hline
              \cellcolor{lightgray}4 & 1 & 1 & 0 &  & 0 & 0\\
              \hline
              \cellcolor{lightgray}... &  &  &  &  &  & \\ 
              \hline
              \cellcolor{lightgray}149532 & 0 & 0 & 0 &  & 0 & 0\\
              \hline
            \end{tabular}
            \caption{Correlation matrix}
            \label{Tab: Correlation matrix}
      \end{center}
\end{table}


\newpage
\textbf{Binary search} is an algorithm that find an element in an array with a complexity of O(log n). It searches in a sorted array by repeatedly dividing the search interval in half. Begin with an interval covering the whole array. If the value of the search key is less than the item in the middle of the interval, narrow the interval to the lower half. Otherwise, narrow it to the upper half. Repeatedly check until the value is found or the interval is empty.
This algorithm is been used to speed up the creation of the training lists with values the indexes they refer to.\\

\subsection{Cosine similarity}
It gives a measure of similarity between two non-zero vectors of an inner product space. It calculate the cosine of the angle($\theta$) between them, which is also the same as the inner product of the same vectors normalized to both have lenght 1. \\
\begin{figure}[ht]
      \begin{center}
            \includegraphics[width=0.6\textwidth]{images/cosine.png}
      \end{center}
\end{figure}\\
Because of that it is bounded in the interval [-1,1] for any angle $\theta$ :
\begin{itemize}
      \item two vectors with the same orientation have a cosine similarity of \textbf{1}
      \item two vectors oriented at right angle relative to each other have a similarity of \textbf{0}
      \item two vectors diametrically opposed have a similarity of \textbf{-1}
\end{itemize}
By using the Euclidean dot product formula:      A $\cdot$ B = $\|A\|$ $\|B\|$ $\cos\theta$ \\
We obtain the cosine similarity formula:\\
\begin{equation}
      S_c(A,B) = \cos\theta = \frac{A \cdot B}{\|A\| \|B\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2}\sqrt{\sum_{i=1}^n B_i^2}}
\end{equation}\\
where $A_i$ anf $B_i$ are components of vector A and B respectively.\\ \\


\subsection{User-User \& Movie-Movie similarity}
For the evaluation of the similarities between movies, it's been used the cosine similarity formula between rows of the \textbf{correlation matrix}. Instead, for the users' similarities, the formula is been applied using rows of the \textbf{ratings matrix} but considering only movies rated by both the users (in order to be considered similar they must have at least five common ratings).\\

\subsection{Clustering}
In order to maintain all the similarities between users and movies to avoid later calculation, user clusters and movie clusters were created.\\
The algorithm used for the building of the user's clusters(analogous for the movie):
\begin{itemize}
    \item Create a list of user id
    \item Take the first element of the list and check which one of the others in the list is similar
    \item Add all the similar found in a new cluster and delete them from the list
    \item Repeat until the list is empty
\end{itemize}
In particular, to store the clusters, two dictionaries have been used: one containing all the user's clusters and one containing all the movie's clusters. Their structure is the same, the key is the number of the cluster and the value is a list containing the ids of all the elements belonging to the cluster. Furthermore, other two dictionaries have been created for maintaining direct access to clusters given the id of an element(user or movie): the key is the id and the value is the number of its cluster. 
\newline
This technique's cheaper from the computation and from the memory point of view and it allows to reduce the workload of the future operations since all the similarities have already been calculated and stored.


\subsection{Collaborative filtering}
Fill the \textbf{ratings matrix}, predicting some missing values given similar behavior between users:
\begin{itemize}
    \item Given a user, take all the similar users in his cluster
    \item First variant: for each movie, calculate the mean rating between all the ratings that are given by similar users and assign it to the user.\\
    Second variant: for each movie, calculate the mean rating between the most common ratings given by similar users and assign it to the user
    \item Repeat for each user
\end{itemize}


\subsection{Content-based filtering}
Given the matrix filled with some values due to the collaborative filtering, try to predict other ratings with a content-based filtering technique between movies:
\begin{itemize}
    \item Take a user (a row of the \textbf{ratings matrix}) and retrieve all the movies' clusters
    \item For each cluster
    \begin{itemize}
          \item Calculate the average rating given by the user (ratings $\ge$ 0.5) only to movies that belong to the cluster;
          \item Insert the average in the position corresponding to movies of the current cluster not rated yet by the user or, if it's not rated but predicted by the collaborative filtering, insert the mean between them
    \end{itemize}
\end{itemize}


\subsection{SVD}
In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any m × n matrix.
\begin{equation}
      S = U\Sigma V^T
\end{equation}
Specifically, the singular value decomposition of an m $\times$ n complex matrix S is a factorization of the form U$\Sigma V^T$, where \textbf{U} is an m $\times$ m complex unitary matrix, \textbf{$\Sigma$} m $\times$ n rectangular diagonal matrix with non-negative real numbers on the diagonal, and \textbf{V} is an n $\times$ n complex unitary matrix.\\
The diagonal entries $\sigma _{i}$ = $\Sigma _{ii}$ of $\Sigma$ are known as the singular values of S. The number of non-zero singular values is equal to the rank of S. The columns of U and the columns of V are called the left-singular vectors and right-singular vectors of S, respectively.\\

\subsubsection{SVT}
The singular value truncation (SVT) uses the idea of SVD to complete the original matrix. In particular, it's a recursive process that calculates the SVD of the matrix and then tries to reduce its rank by modifying the singular values in $\Sigma$. Finally, reconstruct the matrix using the same \textbf{U} and \textbf{V} but with the new $\Sigma$. In the algorithm used the singular values are modified according to an adaptive threshold: its value depends on the number of iterations done. A constant threshold is also possible, for example, threshold = 50 gave good results.\newline

Here are reported the main steps of the algorithm used:

\begin{itemize}
    \item Copy the current matrix (the \textbf{old matrix})
    \item Perform the SVD: \begin{equation}X = U\Sigma V^T\end{equation}
    \item Calculate the threshold for this step: \begin{equation}
        threshold = b * e^{(-k * a)}
    \end{equation}\\
    where k is the number of iterations already done, a = 0.01, b = 200
    \item To each singular value greater than zero subtract the threshold and set to zero the ones that become negative
    \item Build the \textbf{new matrix} performing the SVD using the same U and V obtained by the initial decomposition and the diagonal matrix $\Sigma$ with the new singular values
    \item Update the new matrix obtained in the previous point with the values fixed, before the application of the algorithm: the true ratings and the ones predicted
    \item If there are negative values in the new matrix, set them to zero
    \item Calculate the increment: the difference in module between the \textbf{new matrix} and the \textbf{old matrix}
    \item Repeat until k is lesser than the maximum number of iterations available and the increment is greater than epsilon, where their values are, respectively, 100 and 0.1
\end{itemize}

\subsection{Neural Network}
Instead of using collaborative and content-based filtering, it's also possible to use Neural Networks to do it implicitly. In the second solution provided it's been created a neural network for each user. Each NN has been trained using two different types of data:

\begin{itemize}
    \item Input: a vector for each movie directly rated by the user (the vector correspond to a row of the \textbf{correlation matrix}).\newline
    Output: a vector of 10 positions for each movie, where each position corresponds to a rating (from 0.5 to 5) and with all zero except in the position corresponding to the right rate where there is 1
    \item The same but with movies the users haven't rated. In this case, ratings of similar users are been used
\end{itemize}

\subsubsection{Structure of the neural network}

The structure of the neural networks is made of 3 types of layers:
\begin{itemize}
    \item Input layer: it's the layer that receives the input vector that, in the case considered, is a vector representing a movie given its genres. Thus this layer has 20 neurons, one for each cell of the input.
    \item Hidden layers: the quantity of these layers and their dimensions (how many neurons each layer has) could be any. For the project, it's been decided to use 2 hidden layers with sizes, respectively, 32 and 16.
    \item Output layer: his size depends on the dimension of the output required, since there are 10 possible ratings from 0.5 to 5 the dimension of this layer is 10 and in each position, there is the probability to have the corresponding rate given an input.
\end{itemize}
\begin{figure}[ht]
      \begin{center}
            \includegraphics[width=0.8\textwidth]{images/ANN_img.png}
      \end{center}
      \caption{ANN structure}
      \label{fig: ANN structure}
\end{figure}



\subsubsection{Activation function}
The activation function characterizes neurons in hidden and output layers. Every node in the same layer must have the same activation function, while it could change between layers.
The activation functions used are:

\begin{itemize}
    \item \textbf{Soft max}: it's been used specifically for the output layer since it allows to have a vector with only positive values and with their sum equal to 1, exactly as a probability vector. 
    \begin{equation}\nonumber
        \sigma(z_i) = \frac{\exp{z_i}}{\sum_{j=0}^n \exp{z_i}}
    \end{equation}
    
    Where n is the size of the output layer and $z_i$ is the value corresponding to the i-th rating 
    
    \item \textbf{Tanh}: 
    \begin{equation}\nonumber
        \sigma(z) = \tanh{z} = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
    \end{equation}
    
    \item \textbf{ReLU}:
    \begin{equation}\nonumber
        \sigma(z) = \max(0 , z)
    \end{equation}
    
    \item \textbf{Leaky-ReLU}:
    \begin{equation}\nonumber
        \sigma(z) = \max(0.1z , z)
    \end{equation}
    
    \item \textbf{Swish}:
    \begin{equation}\nonumber
        \sigma(z) = \frac{z}{1 + e^{-z}} = z \times sigmoid(z)
    \end{equation}
\end{itemize}

\subsubsection{Optimization method}
Two different optimizations method have been used for the training of the neural networks:

\begin{itemize}
    \item Mini-batch stochastic gradient descent (MB-SGD)
    \begin{equation}\nonumber
        \theta^{(0)} given
    \end{equation}
    \begin{equation}\nonumber
        for k = 0,1,...,n_{epochs}
    \end{equation}
    \begin{equation}\nonumber
        g^{k} = \frac{1}{|I_k|} \times \sum_{i \in I_k} \nabla_{\theta} J(x_i , y_i , \theta^{(k)})
    \end{equation}
    \begin{equation}\nonumber
        \theta^{(k+1)} = \theta^{(k)} - \lambda_k \times g^{(k)}
    \end{equation}
    
    \item Stochastic gradient descent with momentum
    \begin{equation}\nonumber
        \theta^{(0)} given , v^{(0)} = 0
    \end{equation}
    \begin{equation}\nonumber
        for k = 0,1,...,n_{epochs}
    \end{equation}
    \begin{equation}\nonumber
        g^{k} = \frac{1}{|I_k|} \times \sum_{i \in I_k} \nabla_{\theta} J(x_i , y_i , \theta^{(k)})
    \end{equation}
    \begin{equation}\nonumber
        v^{(k+1)} = \alpha v^{(k)} - \lambda_k \times g^{(k)}
    \end{equation}
    \begin{equation}\nonumber
        \theta^{(k+1)} = \theta^{(k)} +  v^{(k+1)}
    \end{equation}
\end{itemize}

Where ${\theta}$ are the parameters, \textbf{g} is the gradient of the loss function with respect to ${\theta}$ divided by the dimension of the mini-batch \textbf{$I_{k}$}. \newline The mini-batch is a random subset of the inputs. Instead, \textbf{v} is the "velocity" vector, as a matter of fact its value is big if far from the minimum, while its value is small if close to the minimum.

\subsubsection{Loss function}
For this project two different loss functions have been evaluated: 
\begin{itemize}
    \item Mean square error (MSE):
    \begin{equation}\nonumber
        J = \frac{1}{N} \times \sum_{n=1}^{N} (y_i - y_{i,pred})^2
    \end{equation}
    
    \item Cross entropy:
    \begin{equation}\nonumber
        J = -[y \times \ln(y_{pred}) + (1 - y) \times \ln(1 - y_{pred})]
    \end{equation}
\end{itemize}

Where \textbf{N} is the dim of the output vector, \textbf{y} is the true output and \textbf{$y_{pred}$} is  the  prediction.




\subsection{Possible improvements}
The traditional collaborative filtering algorithms produce low temporal diversity, it recommended the same top-N items to users concluding that there was not much difference between an evaluation made many years before and one made recently. In a real scenario it is possible that the preferences of a user could change over time, therefore use the timestamp of the ratings could make a better prediction. At first it requires the traditional collaborative filtering in order to find the k-nearest neighbours for each user. The steps are:
\begin{itemize}
      \item create user and movie cluster with cosine similarity
      \item create a dictionary for each user with, as key, the user id and as value the list of movies ids he evaluate
      \item order the movie list follow a timeline based on the timestamp on which the rating has been made
      \item get one user at time, call it \textsl{‘target’}
      \item from the dictionary extract the timestamp of his last rating (\textsl{t})
      \item get all movie dictionary of all users similar to the target one
      \item select only the movie evaluated by the neighbours from time \textsl{t} or after, that will be considered for the prediction
      \item the ratings prediction of the movie in the new list will be the average of the neighbours rating on that movie
\end{itemize}
In this way the accuracy is higher than the traditional collaborative filtering algorithms, but this approach would also mean losing out on other valid recommendations (excluding all the ratings made before the time ’t’).

\subsection{Assumptions}
During the development of the system, some assumption has been made:
\begin{itemize}
      \item \textbf{Clustering}: to speed up the somputation of the similarity between user and movie, some group of these elements similar has been reated.
      \begin{itemize}
            \item For the clusers containing users, the similarity is evaluated based on the same ratings behaviour. The algorithm generates 161 cluster for the users. 
            \item The other set of clusters contains movies similar for their genres. In this case the amount of clusters is 200.
      \end{itemize} 
      Another possibility could have been to generate a similarity matrix with user-user and item-item as rows-columns, filled with the cosine similarity calculated between each users or movies. This could be more accurate and precise, but makes the algolithm much more slower and computationally heavy.
      \item \textbf{Threshold} for similarity: to evaluate two elements as similar, their cosine similarity have to return a value grather or equal than 0.7 (for the movies) and 0.95 (for the users)
      \item \textbf{Rating recommended}: a movie is recommended by the system if the rating predictes has a value equal or grather than 3.
      \item \textbf{SVT}: the threshold selected to compute the truncation is calculated with two constant a and b, respectively 0.01 and 200, and k, the number of the iteration.
      \item \textbf{Hyperparameters}: to train the neural network the best params found are:
      2000 epochs, using the Gaussian distribution to inizitialize the weights, select the learning rate with the linear decay between a minimum learning rate ${\lambda_{min}}$ = ${e^{-4}}$ and a maximum learning rate ${\lambda_{max}}$ = ${e^{-2}}$ and decay lenght K=1000, implement the stocastic gradient descent with momentum with alpa ${\alpha}$ = 0.95 and minibatch of a size equal to the 30\% of the input size. 
      \item \textbf{Hiddel layer}: the size of the hidden layer of the neural network are 32 and 16 respectivelly.
\end{itemize}



\newpage

%------------------------------------------%
\section{Results}
It's been implemented a function that given a user id retrieve 10 movies the system would recommend. An example is reported below:

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
        \hline
    \rowcolor{lightgray}N° & Title & Rating predicted\\ 
        \hline
        1 & Dream Man (1995) & 5\\
        \hline
        2 & Faster Pussycat! Kill! Kill! (1965) & 5\\
        \hline
        3 & Endless Summer 2, The (1994) & 5\\
        \hline
        4 & Heaven \& Earth (1993) & 5\\
        \hline
        5 & Purple Noon (Plein soleil) (1960) & 5\\
        \hline
        6 & Stonewall (1995) & 5\\
        \hline
        7 & Little Lord Fauntleroy (1936) & 5\\
        \hline
        8 & American Dream (1990) & 5\\
        \hline
        9 & Schizopolis (1996) & 5\\
        \hline
        10 & Quest for Camelot (1998) & 5\\
        \hline
    \end{tabular}
    \caption{Recommend movies for user with id 248 by collaborative and content filtering}
    \label{tab:recommendedMovie1}
\end{table}

Add the same but from the NN

\begin{table}[h]
      \centering
      \begin{tabular}{c|c|c}
          \hline
      \rowcolor{lightgray}N° & Title & Rating predicted\\ 
          \hline
          1 & Shawshank Redemption, The (1994) & 4.5\\
          \hline
          2 & Balto (1995) & 4\\
          \hline
          3 & Power (1995) & 4\\
          \hline
          4 & City of Lost Children, The (Cité des enfants perdus, La) (1995) & 4\\
          \hline
          5 & Twelve Monkeys (a.k.a. 12 Monkeys) (1995) & 4\\
          \hline
          6 & Wings of Courage (1995) & 4\\
          \hline
          7 & Seven (a.k.a. Se7en) (1995) & 4\\
          \hline
          8 & Lamerica (1994) & 4\\
          \hline
          9 & Lawnmower Man 2: Beyond Cyberspace (1996) & 4\\
          \hline
          10 & Screamers (1995) & 4\\
          \hline
      \end{tabular}
      \caption{Recommend movies for user with id 565 with neural network}
      \label{tab:recommendedMovie2}
  \end{table}

\subsection{Evaluation}

\subsubsection{RMSE}

Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how to spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. 

The formula is:
\begin{equation}
      RMSE = \sqrt{\frac{1}{|\Omega_{test}|}\sum_{i,j\in\Omega_test} (r_{i,j} - r_{i,j}^{pred})}
\end{equation}

\subsubsection{Rho}
Pearson correlation coefficient is a measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus it is essentially a normalized measurement of the covariance, such that the result always has a value between −1 and 1. As with covariance itself, the measure can only reflect a linear correlation of variables and ignores many other types of relationship or correlation.
It is calculated as follow:
\begin{equation}
      \rho = \frac{\sum_{\Omega_{test}} (r_{ij}-\overline{r})(r_{ij}^{pred}-\overline{r^{pred}})}{{\sqrt{\sum_{\Omega_{test}}(r_{ij}-\overline{r})^2}}{\sqrt{\sum_{\Omega_{test}}(r_{ij}^{pred}-\overline{r^{pred}})^2}}}
\end{equation}
The result is a value between [-1,1]:
\begin{itemize}
      \item 1 = perfect match-strong positive relation; all data points lie on a line for which both variable r and $r_{pred}$ are increasing
      \item 0 = random; there is no linear dependency between the variables
      \item -1 = strong negative relation; also here a linear equation describes the relationship between the variables r and $r_{pred}$ and the points lie on a line in which are decreasing
\end{itemize}
\begin{figure}[ht]
      \begin{center}
            \includegraphics[width=0.6\textwidth]{images/pearson.png}
      \end{center}
\end{figure}

\subsection{Precision}
Precision tries to answer the following question:\\
'What proportion of positive identification was actually correct?'

\begin{equation}
      Precision = \frac{TP}{TP + FP}
\end{equation}
\subsection{Recall}
Recall tries to answer the following question:\\
'What proportion of actual positives was identified correctly?'

\begin{equation}
      Recall = \frac{TP}{TP + FN}
\end{equation}

\subsection{F1-measure}
It's a measure given by the relation between precision and recall
\begin{equation}
    F1-score = 2 \times \frac{Precision x Recall}{Precision + Recall}
\end{equation}

\subsection{Collaborative \& content-based filtering results}

The results obtained by collaborative filtering, content-based filtering and their combination are reported in table \ref{FilteringResults}.

\begin{table}[h]
      \begin{tabular}{ | c | c | c | c | c | c |} 
        \hline
        \rowcolor{lightgray} Config & RMSE & rho & Precision & Recall & F1-score \\ 
        \hline
         \cellcolor{lightgray} Only Coll.F & 1.347 & 0.279 & 0.874 & 0.768 & 0.817 \\ 
        \hline
        \cellcolor{lightgray} only Con.F & 0.972 & 0.424 & 0.873 & 0.847 & 0.860 \\ 
        \hline
        \cellcolor{lightgray}Coll.F V1 + Con.F & 0.926 & 0.481 & 0.869 & 0.900 & 0.884\\
        \hline
        \cellcolor{lightgray}Coll.F V2 + Con.F & 0.909 & 0.495 & 0.871 & 0.899 & 0.885\\
        \hline
      \end{tabular}
    \caption{Results of collaborative and content-based filtering}
    \label{FilteringResults}
\end{table}

From the results, it's possible to see how the hybrid model is much better than just applying only one filtering technique both in terms of error and F1-score.
\newpage
\subsection{Neural network results}

The results obtained by neural networks are reported in the following tables.\\ \\

\begin{table}[h]
      \begin{tabular}{ | c | c | c | c | c | c | c | c |} 
        \hline
        \rowcolor{lightgray} Act.F & L & Opt & RMSE & rho & Precision & Recall & F1-score \\ 
        \hline
        \cellcolor{lightgray} tanh & Xen & M & 1.106
            & 0.224 & 0.850 & 0.865 & 0.857 \\ 
        \hline
        \cellcolor{lightgray} tanh & Xen & SGD & 0.994
            & 0.415 & 0.882 & 0.812 & 0.846 \\ 
        \hline
        \cellcolor{lightgray} tanh & MSE & M & 1.263
            & 0.428 & 0.920 & 0.670 & 0.775 \\ 
        \hline
        \cellcolor{lightgray} tanh & MSE & SGD & 1.263
            & 0.428 & 0.920 & 0.669 & 0.775 \\ 
        \hline
        \cellcolor{lightgray} ReLU & Xen & M & 1.297
            & 0.183 & 0.854 & 0.749 & 0.798 \\ 
        \hline
        \cellcolor{lightgray} ReLU & Xen & SGD & 1.074
            & 0.438 & 0.907 & 0.710 & 0.796 \\ 
        \hline
        \cellcolor{lightgray} Swish & Xen & M & 1.138
            & 0.208 & 0.861 & 0.803 & 0.831 \\ 
        \hline
        \cellcolor{lightgray} Swish & Xen & SGD & 1.135
            & 0.415 & 0.904 & 0.714 & 0.798 \\ 
        \hline
        \cellcolor{lightgray} Leaky-ReLU & Xen & M & 1.172
            & 0.168 & 0.854 & 0.666 & 0.749 \\ 
        \hline
        \cellcolor{lightgray} Leaky-ReLU & Xen & SGD &        1.043 & 0.428 & 0.901 & 0.726 & 0.804 \\ 
        \hline
      \end{tabular}
    \caption{Results of neural networks}
    \label{FilteringResultsNN}
\end{table}

\textbf{Legend}:
    \begin{itemize}
        \item \textbf{L}: loss function used
        \item \textbf{Xen}: cross entropy
        \item \textbf{MSE}: mean square error
        \item \textbf{Opt}: optimization method used
        \item \textbf{SGD}: stochastic gradient descent method
        \item \textbf{M}: SGD with momentum
    \end{itemize}
Add some comments on the results


\section{Conclusion}
The scope of this project was to built a recommender system to suggest some movie to a user, based on his preferences and the knowledge obtained by the other users. The best approch resulted was ... but also using ... good result were retreived.
The results obtained could be subject to little fluctuations, caused by the randomly choice of the data in the minibatch during the NN training. 

\section{References}
\begin{itemize}
      \item Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman. \textsl{Movie Recommender Systems: Implementation and Performace Evaluation}, Iowa State University.
      \item Mahesh Goyani and Neha Chaurasiya. \textsl{A Review of Movie Recommendation System: Limitations, Survey and Challenges}
      \item Ravee Khandagale (2018). \textsl{Using Filters in TIME-BASED movie Recommender System}
      \item Nematollah Zarmehi and Farokh Marvasti (2017). \textsl{Adaptive Singular Value Thresholding}
\end{itemize}

\end{document}